{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "CREATE_FINAL_DF_WITH_RESAMPLING.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/presidentlines/AvalancheVol3/blob/main/resampled_df.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "666afbb5"
      },
      "source": [
        "# Team AVYULAUNCH\n",
        "#### E R N K L\n",
        "#### v o  a  a e\n",
        "#### e c  t   n e\n",
        "#### r k  h   e\n",
        "#### e f  a\n",
        "#### t o  n\n",
        "#### t r\n",
        "####   d\n",
        "\n",
        "# 11/13/2021\n",
        "### Combine weather and avalanche data!"
      ],
      "id": "666afbb5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0697ca1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "id": "f0697ca1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "cc1a8729",
        "outputId": "61a2e902-5239-48a0-e192-8a3dfc7b3213"
      },
      "source": [
        "pd.set_option('display.max_rows', 20)\n",
        "\n",
        "# Weather from NOAA Database\n",
        "df_weather = pd.read_csv('weather with provo.csv')\n",
        "\n",
        "# Just keeping core features\n",
        "df_weather = df_weather[['NAME', 'DATE', 'PRCP', 'SNWD', 'TMAX', 'TMIN']]\n",
        "\n",
        "# Get month dummy variables\n",
        "df_weather['DATE'] = pd.to_datetime(df_weather['DATE'])\n",
        "df_weather['MONTH'] = pd.DatetimeIndex(df_weather['DATE']).month\n",
        "df_weather['MONTH'] = df_weather['MONTH'].astype(str)\n",
        "# df_weather = pd.get_dummies(df_weather, columns=['MONTH'])\n",
        "\n",
        "\n",
        "# Create Region variable so we can merge with Avalanche dataset\n",
        "def assign_region(name):\n",
        "    if name == \"BEN LOMOND PEAK, UT US\": return \"Ogden\"\n",
        "    if name == \"ALTA, UT US\": return \"Salt Lake\"\n",
        "    if name == \"BEN LOMOND TRAIL, UT US\": return \"Ogden\"\n",
        "    if name == \"MONTE CRISTO, UT US\": return \"Logan\"\n",
        "    if name == \"BUES CANYON UTAH, UT US\": return \"Ogden\"\n",
        "    if name == \"RAY S VALLEY UTAH, UT US\": return \"Uintas\"\n",
        "    if name == \"SNOWBIRD, UT US\": return \"Salt Lake\"\n",
        "    if name == \"PROVO BYU, UT US\": return \"Provo\"\n",
        "\n",
        "# Create snow difference by Weather station NAME\n",
        "df_list = []\n",
        "by_location = df_weather.groupby('NAME')\n",
        "for name, group in by_location:\n",
        "    # Assign each weather station its region\n",
        "    group['Region'] = assign_region(name)\n",
        "    \n",
        "    # Snow depth of the day before minus the current day\n",
        "    group['snow_diff_day'] = group['SNWD'] - group['SNWD'].shift(1)\n",
        "    # Change in snow over the last week\n",
        "    group['snow_diff_week'] = group['SNWD'] - group['SNWD'].shift(7)\n",
        "    \n",
        "    # Binary saying if we got snow from the day before or not\n",
        "    group['got_snow'] = (group['snow_diff_day'] > 0).astype(int) \n",
        "    \n",
        "    # Previous day's TMAX and TMIN\n",
        "    group['prev_day_TMIN'] = group['TMIN'].shift(1)\n",
        "    group['prev_day_TMAX'] = group['TMAX'].shift(1)\n",
        "    \n",
        "    df_list.append(group)\n",
        "df_weather = pd.concat(df_list)\n",
        "\n",
        "# Create indicator for if it was below freezing at any point that day\n",
        "df_weather['min_below_freezing'] = (df_weather['TMIN'] < 32).astype(int)\n",
        "# Create indicator for if it was above freezing at any point that day\n",
        "df_weather['max_above_freezing'] = (df_weather['TMAX'] > 32).astype(int)\n",
        "\n",
        "# min * max means:\n",
        "# 1 if min below freezing and max above freezing\n",
        "# 0 otherwise\n",
        "# This is potentially significant if we cross the freezing point of water in a day\n",
        "df_weather['min*max'] = df_weather['min_below_freezing'] * df_weather['max_above_freezing']\n",
        "\n",
        "\n",
        "\n",
        "print(df_weather.columns)"
      ],
      "id": "cc1a8729",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['NAME', 'DATE', 'PRCP', 'SNWD', 'TMAX', 'TMIN', 'MONTH', 'Region',\n",
            "       'snow_diff_day', 'snow_diff_week', 'got_snow', 'prev_day_TMIN',\n",
            "       'prev_day_TMAX', 'min_below_freezing', 'max_above_freezing', 'min*max'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4345c436",
        "outputId": "e8e0af92-7e1c-4c63-ab2e-4163262f12cd"
      },
      "source": [
        "# Clean up avalanche data\n",
        "df_avalanche = pd.read_csv('avalanches, 11-13-2021.csv')\n",
        "# Delete rows without date or region\n",
        "df_avalanche = df_avalanche[['Date', 'Region']].dropna()\n",
        "# Convert to date time\n",
        "df_avalanche['DATE'] = pd.to_datetime(df_avalanche['Date'])\n",
        "df_avalanche['Avalanche'] = 1\n",
        "\n",
        "print(df_avalanche.columns)"
      ],
      "id": "4345c436",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Date', 'Region', 'DATE', 'Avalanche'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01f79f4c"
      },
      "source": [
        "# Combine avalanche and weather on region and date\n",
        "df_combined = pd.merge(df_weather, df_avalanche,  how='left', on=['Region', 'DATE'])"
      ],
      "id": "01f79f4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f471cc7",
        "outputId": "5e4d3e24-9eb8-4c86-fcb0-1fdc5ff4d1b7"
      },
      "source": [
        "# Create variable of summed up avalanches per day by Weather Station NAME\n",
        "summed = df_combined.groupby(['NAME', 'DATE'])['Avalanche'].agg('sum').reset_index()\n",
        "summed['avalanche_sum'] = summed['Avalanche']\n",
        "\n",
        "# Add new column back onto original dataframe\n",
        "reassembled = pd.merge(summed, df_combined, how='left', on=['NAME', 'DATE'])\n",
        "reassembled = reassembled.drop_duplicates()\n",
        "\n",
        "# Create new column: binary indicator if there was an avalanche that day or not\n",
        "reassembled['avalanche_binary'] = reassembled['avalanche_sum'] > 0\n",
        "\n",
        "# Final clean up: drop unnecessary columns\n",
        "df_final = reassembled.drop(['Avalanche_x', 'Avalanche_y', 'Date', 'NAME'], axis=1)\n",
        "\n",
        "# Create region dummy variables\n",
        "df_final = pd.get_dummies(df_final, columns=['Region'], drop_first=True)\n",
        "\n",
        "# Drop rows with nan variables\n",
        "df_final = df_final.dropna()\n",
        "print(df_final.columns)\n",
        "\n",
        "df_final = df_final[df_final['DATE'] > '2010-01-01']\n",
        "\n",
        "df_final.to_csv(\"FINAL_DF.csv\")"
      ],
      "id": "7f471cc7",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['DATE', 'avalanche_sum', 'PRCP', 'SNWD', 'TMAX', 'TMIN', 'MONTH',\n",
            "       'snow_diff_day', 'snow_diff_week', 'got_snow', 'prev_day_TMIN',\n",
            "       'prev_day_TMAX', 'min_below_freezing', 'max_above_freezing', 'min*max',\n",
            "       'avalanche_binary', 'Region_Ogden', 'Region_Provo', 'Region_Salt Lake',\n",
            "       'Region_Uintas'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cabc4d11"
      },
      "source": [
        "################## UPSAMPLING / DOWNSAMPLING ##################\n",
        "\n",
        "# If there are 3,000 weather observations and 15 avalanche dates, \n",
        "# we will have 99.5% accuracy with a model that just classifies\n",
        "# everything as No Avalanche. So, we'll downsample our observations\n",
        "# so we have as many weather observations for non-avalanche days\n",
        "# as we have for avalanche days. \n",
        "# If there aren't over 100 observations for avalanche days, we will\n",
        "# upsample those and take a random draw of 100 from those observations.\n",
        "\n",
        "# Potential here for augmenting those data points slightly. \n",
        "# We'd only want to slightly perturb the following columns:\n",
        "# 'PRCP', 'SNWD', 'TMAX', 'TMIN', 'snow_diff_day', 'snow_diff_week'\n",
        "# We'd just want to be careful of TMAX or TMIN being around 32 since\n",
        "# that could throw off our data. \n",
        "# Another idea: only changing the region of the observations, seeing\n",
        "# if that affects anything. \n",
        "\n",
        "group_list = []\n",
        "for month, group in df_final.groupby('MONTH'):\n",
        "    # Separate avalanched days from non avalanche days\n",
        "    avi_days = group[group['avalanche_binary'] == 1]\n",
        "    non_avi_days = group[group['avalanche_binary'] == 0]\n",
        "    \n",
        "    # If there aren't very many avalanche days, upsample those\n",
        "    num_samples = len(avi_days)\n",
        "    if num_samples < 100:\n",
        "        num_samples = 100\n",
        "        \n",
        "    # Check if we have any avalanches that month\n",
        "    # otherwise we can leave those months out entirely\n",
        "    if len(avi_days) > 0:\n",
        "        # Get random sample of size num_samples which is max(100, # of avalanches)\n",
        "        avi_sample_index = np.random.choice(avi_days.index, num_samples)\n",
        "        # Add those to our new dataframe\n",
        "        group_list.append(avi_days.loc[avi_sample_index])\n",
        "        \n",
        "        #  If we take these two lines out of this if statement,\n",
        "        # we end up adding 100 rows to our dataframe from the \n",
        "        # summer months where we never have avalanches\n",
        "        non_avi_sample_index = np.random.choice(non_avi_days.index, num_samples, replace=False)\n",
        "        group_list.append(non_avi_days.loc[non_avi_sample_index])\n",
        "    \n",
        "# Compile new dataframe from each month's sample\n",
        "df_resampled = pd.concat(group_list)"
      ],
      "id": "cabc4d11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "263bc882"
      },
      "source": [
        "df = pd.get_dummies(df_resampled, columns=['MONTH'], drop_first=True)\n",
        "df.to_csv(\"RESAMPLED_DF.csv\")"
      ],
      "id": "263bc882",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c43eafb4",
        "outputId": "54c5253f-adbe-4c6c-c9b7-eaf5c08185f4"
      },
      "source": [
        "# LOGISTIC REGRESSION\n",
        "X = df_final.drop(['avalanche_sum', 'avalanche_binary', 'DATE'], axis=1)\n",
        "y = df_final['avalanche_binary']\n",
        "print(\"% observations w/o avalanches:\", round((len(X) - sum(y)) * 100 / len(X), 1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(random_state=0, max_iter=1e4)\n",
        "clf.fit(X_train, y_train)\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "print(\"% predictions of an avalanche:\", round(sum(y_hat) * 100 / len(y_hat), 1))\n",
        "print(\"% correct predictions:\", round(sum(y_hat == y_test) * 100 / len(y_hat), 1))\n",
        "print(\"# of avalanche predictions:\", sum(y_hat))"
      ],
      "id": "c43eafb4",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "% observations w/o avalanches: 88.3\n",
            "% predictions of an avalanche: 6.8\n",
            "% correct predictions: 90.6\n",
            "# of avalanche predictions: 335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7a35f0c",
        "outputId": "556ac868-f616-44cf-c95a-bb7404efec60"
      },
      "source": [
        "# LOGISTIC REGRESSION\n",
        "X = df.drop(['avalanche_sum', 'avalanche_binary', 'DATE'], axis=1)\n",
        "y = df['avalanche_binary']\n",
        "print(\"% observations w/o avalanches:\", round((len(X) - sum(y)) * 100 / len(X), 1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(random_state=0, max_iter=1e4)\n",
        "clf.fit(X_train, y_train)\n",
        "y_hat = clf.predict(X_test)\n",
        "\n",
        "print(\"% predictions of an avalanche:\", round(sum(y_hat) * 100 / len(y_hat), 1))\n",
        "print(\"% correct predictions:\", round(sum(y_hat == y_test) * 100 / len(y_hat), 1))\n",
        "print(\"# of avalanche predictions:\", sum(y_hat))"
      ],
      "id": "b7a35f0c",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "% observations w/o avalanches: 52.3\n",
            "% predictions of an avalanche: 46.1\n",
            "% correct predictions: 74.1\n",
            "# of avalanche predictions: 598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d9a47a7"
      },
      "source": [
        ""
      ],
      "id": "6d9a47a7",
      "execution_count": null,
      "outputs": []
    }
  ]
}