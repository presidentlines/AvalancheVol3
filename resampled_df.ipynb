{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3cd2020",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/presidentlines/AvalancheVol3/blob/main/resampled_df.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666afbb5",
   "metadata": {
    "id": "666afbb5"
   },
   "source": [
    "# Team AVYULAUNCH\n",
    "\n",
    "# 11/13/2021\n",
    "### Combine weather and avalanche data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0697ca1",
   "metadata": {
    "id": "f0697ca1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f471cc7",
   "metadata": {
    "id": "7f471cc7",
    "outputId": "5e4d3e24-9eb8-4c86-fcb0-1fdc5ff4d1b7"
   },
   "outputs": [],
   "source": [
    "def get_resampled_df():\n",
    "    pd.set_option('display.max_rows', 20)\n",
    "\n",
    "    # Weather from NOAA Database\n",
    "    df_weather = pd.read_csv('weather with provo.csv')\n",
    "\n",
    "    # Just keeping core features\n",
    "    df_weather = df_weather[['NAME', 'DATE', 'PRCP', 'SNWD', 'TMAX', 'TMIN']]\n",
    "\n",
    "    # Get month dummy variables\n",
    "    df_weather['DATE'] = pd.to_datetime(df_weather['DATE'])\n",
    "    df_weather['MONTH'] = pd.DatetimeIndex(df_weather['DATE']).month\n",
    "    df_weather['MONTH'] = df_weather['MONTH'].astype(str)\n",
    "    # df_weather = pd.get_dummies(df_weather, columns=['MONTH'])\n",
    "\n",
    "\n",
    "    # Create Region variable so we can merge with Avalanche dataset\n",
    "    def assign_region(name):\n",
    "        if name == \"BEN LOMOND PEAK, UT US\": return \"Ogden\"\n",
    "        if name == \"ALTA, UT US\": return \"Salt Lake\"\n",
    "        if name == \"BEN LOMOND TRAIL, UT US\": return \"Ogden\"\n",
    "        if name == \"MONTE CRISTO, UT US\": return \"Logan\"\n",
    "        if name == \"BUES CANYON UTAH, UT US\": return \"Ogden\"\n",
    "        if name == \"RAY S VALLEY UTAH, UT US\": return \"Uintas\"\n",
    "        if name == \"SNOWBIRD, UT US\": return \"Salt Lake\"\n",
    "        if name == \"PROVO BYU, UT US\": return \"Provo\"\n",
    "\n",
    "    # Create snow difference by Weather station NAME\n",
    "    df_list = []\n",
    "    by_location = df_weather.groupby('NAME')\n",
    "    for name, group in by_location:\n",
    "        # Assign each weather station its region\n",
    "        group['Region'] = assign_region(name)\n",
    "\n",
    "        # Snow depth of the day before minus the current day\n",
    "        group['snow_diff_day'] = group['SNWD'] - group['SNWD'].shift(1)\n",
    "        # Change in snow over the last week\n",
    "        group['snow_diff_week'] = group['SNWD'] - group['SNWD'].shift(7)\n",
    "\n",
    "        # Binary saying if we got snow from the day before or not\n",
    "        group['got_snow'] = (group['snow_diff_day'] > 0).astype(int) \n",
    "\n",
    "        # Previous day's TMAX and TMIN\n",
    "        group['prev_day_TMIN'] = group['TMIN'].shift(1)\n",
    "        group['prev_day_TMAX'] = group['TMAX'].shift(1)\n",
    "\n",
    "        df_list.append(group)\n",
    "    df_weather = pd.concat(df_list)\n",
    "\n",
    "    # Create indicator for if it was below freezing at any point that day\n",
    "    df_weather['min_below_freezing'] = (df_weather['TMIN'] < 32).astype(int)\n",
    "    # Create indicator for if it was above freezing at any point that day\n",
    "    df_weather['max_above_freezing'] = (df_weather['TMAX'] > 32).astype(int)\n",
    "\n",
    "    # min * max means:\n",
    "    # 1 if min below freezing and max above freezing\n",
    "    # 0 otherwise\n",
    "    # This is potentially significant if we cross the freezing point of water in a day\n",
    "    df_weather['min*max'] = df_weather['min_below_freezing'] * df_weather['max_above_freezing']\n",
    "\n",
    "\n",
    "\n",
    "    print(df_weather.columns)\n",
    "\n",
    "    # Clean up avalanche data\n",
    "    df_avalanche = pd.read_csv('final_avalanche.csv')\n",
    "    # Delete rows without date or region\n",
    "    df_avalanche = df_avalanche[['Date', 'Region']].dropna()\n",
    "    # Convert to date time\n",
    "    df_avalanche['DATE'] = pd.to_datetime(df_avalanche['Date'])\n",
    "    df_avalanche['Avalanche'] = 1\n",
    "\n",
    "    print(df_avalanche.columns)\n",
    "\n",
    "    #Combine avalanche and weather on region and date\n",
    "    df_combined = pd.merge(df_weather, df_avalanche,  how='left', on=['Region', 'DATE'])\n",
    "\n",
    "    # Create variable of summed up avalanches per day by Weather Station NAME\n",
    "    summed = df_combined.groupby(['NAME', 'DATE'])['Avalanche'].agg('sum').reset_index()\n",
    "    summed['avalanche_sum'] = summed['Avalanche']\n",
    "\n",
    "    # Add new column back onto original dataframe\n",
    "    reassembled = pd.merge(summed, df_combined, how='left', on=['NAME', 'DATE'])\n",
    "    reassembled = reassembled.drop_duplicates()\n",
    "\n",
    "    # Create new column: binary indicator if there was an avalanche that day or not\n",
    "    reassembled['avalanche_binary'] = reassembled['avalanche_sum'] > 0\n",
    "\n",
    "    # Final clean up: drop unnecessary columns\n",
    "    df_final = reassembled.drop(['Avalanche_x', 'Avalanche_y', 'Date', 'NAME'], axis=1)\n",
    "\n",
    "    # Create region dummy variables\n",
    "    df_final = pd.get_dummies(df_final, columns=['Region'], drop_first=True)\n",
    "\n",
    "    # Drop rows with nan variables\n",
    "    df_final = df_final.dropna()\n",
    "    print(df_final.columns)\n",
    "\n",
    "    df_final = df_final[df_final['DATE'] > '2010-01-01']\n",
    "\n",
    "    df_final.to_csv(\"FINAL_DF.csv\")\n",
    "\n",
    "    ################## UPSAMPLING / DOWNSAMPLING ##################\n",
    "\n",
    "    # If there are 3,000 weather observations and 15 avalanche dates, \n",
    "    # we will have 99.5% accuracy with a model that just classifies\n",
    "    # everything as No Avalanche. So, we'll downsample our observations\n",
    "    # so we have as many weather observations for non-avalanche days\n",
    "    # as we have for avalanche days. \n",
    "    # If there aren't over 100 observations for avalanche days, we will\n",
    "    # upsample those and take a random draw of 100 from those observations.\n",
    "\n",
    "    # Potential here for augmenting those data points slightly. \n",
    "    # We'd only want to slightly perturb the following columns:\n",
    "    # 'PRCP', 'SNWD', 'TMAX', 'TMIN', 'snow_diff_day', 'snow_diff_week'\n",
    "    # We'd just want to be careful of TMAX or TMIN being around 32 since\n",
    "    # that could throw off our data. \n",
    "    # Another idea: only changing the region of the observations, seeing\n",
    "    # if that affects anything. \n",
    "\n",
    "    group_list = []\n",
    "    for month, group in df_final.groupby('MONTH'):\n",
    "        # Separate avalanched days from non avalanche days\n",
    "        avi_days = group[group['avalanche_binary'] == 1]\n",
    "        non_avi_days = group[group['avalanche_binary'] == 0]\n",
    "\n",
    "        # If there aren't very many avalanche days, upsample those\n",
    "        num_samples = len(avi_days)\n",
    "        if num_samples < 100:\n",
    "            num_samples = 100\n",
    "\n",
    "        # Check if we have any avalanches that month\n",
    "        # otherwise we can leave those months out entirely\n",
    "        if len(avi_days) > 0:\n",
    "            # Get random sample of size num_samples which is max(100, # of avalanches)\n",
    "            avi_sample_index = np.random.choice(avi_days.index, num_samples)\n",
    "            # Add those to our new dataframe\n",
    "            group_list.append(avi_days.loc[avi_sample_index])\n",
    "\n",
    "            #  If we take these two lines out of this if statement,\n",
    "            # we end up adding 100 rows to our dataframe from the \n",
    "            # summer months where we never have avalanches\n",
    "            non_avi_sample_index = np.random.choice(non_avi_days.index, num_samples, replace=False)\n",
    "            group_list.append(non_avi_days.loc[non_avi_sample_index])\n",
    "\n",
    "    # Compile new dataframe from each month's sample\n",
    "    df_resampled = pd.concat(group_list)\n",
    "    return df_resampled, df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc4d11",
   "metadata": {
    "id": "cabc4d11"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "263bc882",
   "metadata": {
    "id": "263bc882",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['NAME', 'DATE', 'PRCP', 'SNWD', 'TMAX', 'TMIN', 'MONTH', 'Region',\n",
      "       'snow_diff_day', 'snow_diff_week', 'got_snow', 'prev_day_TMIN',\n",
      "       'prev_day_TMAX', 'min_below_freezing', 'max_above_freezing', 'min*max'],\n",
      "      dtype='object')\n",
      "Index(['Date', 'Region', 'DATE', 'Avalanche'], dtype='object')\n",
      "Index(['DATE', 'avalanche_sum', 'PRCP', 'SNWD', 'TMAX', 'TMIN', 'MONTH',\n",
      "       'snow_diff_day', 'snow_diff_week', 'got_snow', 'prev_day_TMIN',\n",
      "       'prev_day_TMAX', 'min_below_freezing', 'max_above_freezing', 'min*max',\n",
      "       'avalanche_binary', 'Region_Ogden', 'Region_Provo', 'Region_Salt Lake',\n",
      "       'Region_Uintas'],\n",
      "      dtype='object')\n",
      "            DATE  avalanche_sum  PRCP  SNWD  TMAX  TMIN MONTH  snow_diff_day  \\\n",
      "64877 2015-01-24            2.0   0.0  51.0  36.0  22.0     1            0.0   \n",
      "13580 2010-01-31            4.0   1.5  71.0  26.0  24.0     1            7.0   \n",
      "65379 2016-01-22            3.0   0.0  54.0  42.0  30.0     1           -2.0   \n",
      "67432 2020-01-14            3.0   0.4  81.0  26.0  13.0     1            1.0   \n",
      "46452 2011-01-20            1.0   0.0   0.0  38.0  27.0     1            0.0   \n",
      "...          ...            ...   ...   ...   ...   ...   ...            ...   \n",
      "67143 2019-06-02            0.0   0.0  77.0  60.0  39.0     6           -2.0   \n",
      "14473 2012-06-14            0.0   0.0   0.0  67.0  49.0     6            0.0   \n",
      "13735 2010-06-22            0.0   0.0   0.0  60.0  47.0     6            0.0   \n",
      "8376  2019-06-10            0.0   0.0  20.0  61.0  35.0     6           -4.0   \n",
      "6822  2016-06-25            0.0   0.0   0.0  67.0  39.0     6            0.0   \n",
      "\n",
      "       snow_diff_week  got_snow  prev_day_TMIN  prev_day_TMAX  \\\n",
      "64877            -3.0         0           21.0           32.0   \n",
      "13580             4.0         1           21.0           33.0   \n",
      "65379            -1.0         0           15.0           38.0   \n",
      "67432            28.0         1           17.0           26.0   \n",
      "46452            -4.0         0           34.0           41.0   \n",
      "...               ...       ...            ...            ...   \n",
      "67143           -12.0         0           37.0           59.0   \n",
      "14473             0.0         0           52.0           67.0   \n",
      "13735             0.0         0           44.0           63.0   \n",
      "8376            -24.0         0           25.0           50.0   \n",
      "6822              0.0         0           48.0           72.0   \n",
      "\n",
      "       min_below_freezing  max_above_freezing  min*max  avalanche_binary  \\\n",
      "64877                   1                   1        1              True   \n",
      "13580                   1                   0        0              True   \n",
      "65379                   1                   1        1              True   \n",
      "67432                   1                   0        0              True   \n",
      "46452                   1                   1        1              True   \n",
      "...                   ...                 ...      ...               ...   \n",
      "67143                   0                   1        0             False   \n",
      "14473                   0                   1        0             False   \n",
      "13735                   0                   1        0             False   \n",
      "8376                    0                   1        0             False   \n",
      "6822                    0                   1        0             False   \n",
      "\n",
      "       Region_Ogden  Region_Provo  Region_Salt Lake  Region_Uintas  \n",
      "64877             0             0                 1              0  \n",
      "13580             1             0                 0              0  \n",
      "65379             0             0                 1              0  \n",
      "67432             0             0                 1              0  \n",
      "46452             0             1                 0              0  \n",
      "...             ...           ...               ...            ...  \n",
      "67143             0             0                 1              0  \n",
      "14473             1             0                 0              0  \n",
      "13735             1             0                 0              0  \n",
      "8376              0             0                 1              0  \n",
      "6822              0             0                 1              0  \n",
      "\n",
      "[6188 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "df_resampled = get_resampled_df()[0]\n",
    "df = pd.get_dummies(df_resampled, columns=['MONTH'], drop_first=True)\n",
    "df.to_csv(\"RESAMPLED_DF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c43eafb4",
   "metadata": {
    "id": "c43eafb4",
    "outputId": "54c5253f-adbe-4c6c-c9b7-eaf5c08185f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['NAME', 'DATE', 'PRCP', 'SNWD', 'TMAX', 'TMIN', 'MONTH', 'Region',\n",
      "       'snow_diff_day', 'snow_diff_week', 'got_snow', 'prev_day_TMIN',\n",
      "       'prev_day_TMAX', 'min_below_freezing', 'max_above_freezing', 'min*max'],\n",
      "      dtype='object')\n",
      "Index(['Date', 'Region', 'DATE', 'Avalanche'], dtype='object')\n",
      "Index(['DATE', 'avalanche_sum', 'PRCP', 'SNWD', 'TMAX', 'TMIN', 'MONTH',\n",
      "       'snow_diff_day', 'snow_diff_week', 'got_snow', 'prev_day_TMIN',\n",
      "       'prev_day_TMAX', 'min_below_freezing', 'max_above_freezing', 'min*max',\n",
      "       'avalanche_binary', 'Region_Ogden', 'Region_Provo', 'Region_Salt Lake',\n",
      "       'Region_Uintas'],\n",
      "      dtype='object')\n",
      "            DATE  avalanche_sum  PRCP  SNWD  TMAX  TMIN MONTH  snow_diff_day  \\\n",
      "68030 2021-01-04            1.0   0.1  30.0  39.0  21.0     1            0.0   \n",
      "39727 2015-01-06            1.0   0.0  40.0  44.0  31.0     1            1.0   \n",
      "67474 2020-01-28            4.0   0.1  87.0  30.0  22.0     1           -1.0   \n",
      "37753 2010-01-07            1.0   0.0  33.0  25.0   8.0     1            0.0   \n",
      "50169 2021-01-08            1.0   0.0   0.0  43.0  23.0     1            0.0   \n",
      "...          ...            ...   ...   ...   ...   ...   ...            ...   \n",
      "9749  2021-06-22            0.0   0.0   0.0  75.0  51.0     6            0.0   \n",
      "9754  2021-06-27            0.0   0.0   0.0  67.0  43.0     6            0.0   \n",
      "24783 2018-06-26            0.0   0.0   0.0  78.0  42.0     6            0.0   \n",
      "48860 2017-06-30            0.0   0.0   0.0  87.0  55.0     6            0.0   \n",
      "13722 2010-06-09            0.0   0.2   0.0  62.0  46.0     6            0.0   \n",
      "\n",
      "       snow_diff_week  got_snow  prev_day_TMIN  prev_day_TMAX  \\\n",
      "68030             0.0         0           23.0           27.0   \n",
      "39727            -8.0         1           24.0           32.0   \n",
      "67474             8.0         0           20.0           27.0   \n",
      "37753             0.0         0           13.0           28.0   \n",
      "50169             0.0         0           23.0           45.0   \n",
      "...               ...       ...            ...            ...   \n",
      "9749              0.0         0           45.0           74.0   \n",
      "9754              0.0         0           43.0           64.0   \n",
      "24783             0.0         0           42.0           70.0   \n",
      "48860             0.0         0           58.0           89.0   \n",
      "13722            -3.0         0           44.0           62.0   \n",
      "\n",
      "       min_below_freezing  max_above_freezing  min*max  avalanche_binary  \\\n",
      "68030                   1                   1        1              True   \n",
      "39727                   1                   1        1              True   \n",
      "67474                   1                   0        0              True   \n",
      "37753                   1                   0        0              True   \n",
      "50169                   1                   1        1              True   \n",
      "...                   ...                 ...      ...               ...   \n",
      "9749                    0                   1        0             False   \n",
      "9754                    0                   1        0             False   \n",
      "24783                   0                   1        0             False   \n",
      "48860                   0                   1        0             False   \n",
      "13722                   0                   1        0             False   \n",
      "\n",
      "       Region_Ogden  Region_Provo  Region_Salt Lake  Region_Uintas  \n",
      "68030             0             0                 1              0  \n",
      "39727             0             0                 0              0  \n",
      "67474             0             0                 1              0  \n",
      "37753             0             0                 0              0  \n",
      "50169             0             1                 0              0  \n",
      "...             ...           ...               ...            ...  \n",
      "9749              0             0                 1              0  \n",
      "9754              0             0                 1              0  \n",
      "24783             1             0                 0              0  \n",
      "48860             0             1                 0              0  \n",
      "13722             1             0                 0              0  \n",
      "\n",
      "[6188 rows x 20 columns]\n",
      "% observations w/o avalanches: 88.3\n",
      "% predictions of an avalanche: 6.8\n",
      "% correct predictions: 90.5\n",
      "# of avalanche predictions: 334\n"
     ]
    }
   ],
   "source": [
    "df_final = get_resampled_df()[1]\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "X = df_final.drop(['avalanche_sum', 'avalanche_binary', 'DATE'], axis=1)\n",
    "y = df_final['avalanche_binary']\n",
    "print(\"% observations w/o avalanches:\", round((len(X) - sum(y)) * 100 / len(X), 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=1e4)\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "print(\"% predictions of an avalanche:\", round(sum(y_hat) * 100 / len(y_hat), 1))\n",
    "print(\"% correct predictions:\", round(sum(y_hat == y_test) * 100 / len(y_hat), 1))\n",
    "print(\"# of avalanche predictions:\", sum(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7a35f0c",
   "metadata": {
    "id": "b7a35f0c",
    "outputId": "556ac868-f616-44cf-c95a-bb7404efec60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% observations w/o avalanches: 50.0\n",
      "% predictions of an avalanche: 48.9\n",
      "% correct predictions: 72.6\n",
      "# of avalanche predictions: 606\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION\n",
    "X = df.drop(['avalanche_sum', 'avalanche_binary', 'DATE'], axis=1)\n",
    "y = df['avalanche_binary']\n",
    "print(\"% observations w/o avalanches:\", round((len(X) - sum(y)) * 100 / len(X), 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=1e4)\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "print(\"% predictions of an avalanche:\", round(sum(y_hat) * 100 / len(y_hat), 1))\n",
    "print(\"% correct predictions:\", round(sum(y_hat == y_test) * 100 / len(y_hat), 1))\n",
    "print(\"# of avalanche predictions:\", sum(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a47a7",
   "metadata": {
    "id": "6d9a47a7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "CREATE_FINAL_DF_WITH_RESAMPLING.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
